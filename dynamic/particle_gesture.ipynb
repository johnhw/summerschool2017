{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imgs/logo.png\" width=\"100%\">\n",
    "-----\n",
    "\n",
    "## Probabilistic filtering for intention inference\n",
    "\n",
    "### Particle filters: Sequential Monte Carlo estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Problem description\n",
    "We are going to solve a simple problem:\n",
    "\n",
    "* Recognising simple 2D gestures, drawn with a mouse.\n",
    "\n",
    "This is readily solved with \"standard\" machine learning algorithms, but we will show how a model-led approach lets us encode our assumptions elegantly **and** we get all the benefits of *probabilistic* tracking. We'll see how probabilistic filters degrade gracefully when our models are bad or measurements are noisy.\n",
    "\n",
    "We are going to view the problem as inferring which *class* a time series of observations belongs to. This is a very general problem in HCI; by explicitly treating this a **process** rather than as a static **classification** problem, we can resolve much of the clumsiness of many gesture recognisers.\n",
    "\n",
    "<img  src=\"imgs/capture.png\" width=\"80%\"/>\n",
    "\n",
    "#### Algorithm\n",
    "We will use the **particle filter** algorithm (technically the **SIR** variant, which is the simplest to understand).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why particle filter?\n",
    "The Kalman filter used the normal distribution to model all of the uncertainty in the system. This is great for computational efficiency, since the updates are simple linear transforms. It is also inferentially efficient; given only a small amount of evidence the Kalman filter will converge quickly compared to other approaches.\n",
    "\n",
    "But it has several significant drawbacks, which make it difficult to apply directly to infer *gestures* from observations:\n",
    "\n",
    "#### Kalman filter drawbacks\n",
    "* the **dynamics** have to be linear: we can't have complicated dynamic models (although we can linearise at each time step).\n",
    "\n",
    "This doesn't make much sense for tracking complex gesture trajectories; a dynamic model for a complete gesture is rarely going to be linear. We might want to be able to learn complex dynamics using a deep network, for example, and then plug them into a probabilistic filter. A Kalman filter does not support this.\n",
    "\n",
    "* all of the **uncertainty** must be normal: so we can't track multiple modes, for example, because a normal distribution has exactly one mode. \n",
    "\n",
    "Imagine an object disappearing behind an obstruction which could reappear on either side; the Kalman filter can only spread out the distribution over the whole area, with an expected location in the middle of the obstacle! We would like to instead be able to track the two possibilities here by splitting up the hypotheses. \n",
    "\n",
    "<img src=\"imgs/landscape.png\">\n",
    "*[Waddington's epigenetic landscape, illustrating a dynamic system which develops multiple modes as it evolves; a Gaussian approximation is wholly inappropriate]*\n",
    "\n",
    "Very often in HCI we encounter problems with a combination of discrete variables and continuous ones.\n",
    "This is critical in gesture recognition for example; at any point in time, our hypotheses might be split among multiple possible gestures with different spatial distributions. Being able to represent the combination of discrete + continuous states is critical. (**Kalman filter banks** are an alternative approach, which explicitly maintain the competing hypotheses as separate Kalman filters).\n",
    "\n",
    "As an aside, the **hidden Markov model**, formerly a key algorithm in speech recognition, is to discrete state tracking what the Kalman filter is to continuous state tracking. The HMM can track discrete hidden states easily (with discrete or continuous observation space), but cannot track continuous variables on its own.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "### Particle Filtering \n",
    "Particle filters are very simple to understand:\n",
    "\n",
    "* **Sample-based:** Instead of trying to represent the PDFs of our distributions, which could be any function that integrates to unity, we represent **distributions** using **samples** from those distributions (i.e. it is a **sequential Monte Carlo** method; we use samples to represent distributions). We choose some fixed number of particles $N_s$ and have a set of particles $S_t = \\{ x^{1}_t, x^{2}_t, \\dots, x^{N_s}_t \\}$. At any time point, our belief is captured by this sample set.\n",
    "* **Arbitrary dynamics:** we can apply **any** dynamics simply by applying our transition function $f({\\bf x_t})$ to each sample from our current set $S_t$ to get a new predicted set of samples $S_{t+1}$. Likewise, for any sample, we can compute the corresponding observation by applying an arbitrary function $g(x_t)$ to each sample independently. There are **no** restrictions on the form of $f({\\bf x_t})$ and $g({\\bf x_t})$.\n",
    "* **Arbitrary distribution:** Our uncertainty is captured by the locations of the set of particles in the state space; they are samples from our current posterior. We do not have an explicit parametric form for the distribution; samples are all that we have. Therefore there are no restrictions on the form of the distribution; it can be multi-modal, heavy-tailed etc. \n",
    "* **No explicit likelihood:** We don't explicitly compute the likelihood of observations, but instead we apply some weighting function to the particles and then normalise these weights to approximate the likelihood. This means we only have to find a reasonable weighting function, and not a true likelihood.\n",
    "* **Importance sampled** We **resample** particles which are likely after an observation, and discard those which are unlikely. This is **importance resampling**. Without this step, particles would quickly diffuse into very unlikely parts of the space. **Importance resampling** continuously adapts the particles to cover likely portions of the space.\n",
    "\n",
    "**This can only ever approximate the \"true\" distribution; but the Monte Carlo approximation turns out to work surprisingly well, and has good theoretical guarantees. **\n",
    "\n",
    "<img src=\"imgs/particleprocess.png\">\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "<video src=\"videos/anim.mpg.webm\">\n",
    "\n",
    "\n",
    "<video src=\"videos/leafv.mpg.webm\">\n",
    "\n",
    "---\n",
    "\n",
    "It might seem surprising that would work, but in fact for many problems it works very well, and it is statistically sound (under relatively relaxed conditions). It is less computationally efficient than the Kalman filter, and also less inferentially efficient (it is usually less certain for the same amount of data). \n",
    "\n",
    "It has many, many variants and many parameters that can be tweaked. This is flexible, but can be hard to optimize. \n",
    "\n",
    "However, it is very easy to implement and its form makes it easy to adapt into other parts of an interface. For example, to compute how likely it is that a user wants to activate a target (in a cursor tracking example) we can simply count all of the particles that fall into the target bounding box; no integration required!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Particle filters have many uses in HCI. For example, we have used them extensively to track finger configurations when using capacitive sensors. In this case, we have a finger pose state space (hidden) and a sensor matrix (observed), and the filter estimates pose in real-time.\n",
    "\n",
    "<img src=\"imgs/anglepose.jpg\">\n",
    "[See the AnglePose video](http://www.dcs.gla.ac.uk/~jhw/AnglePose-final.mov)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specifying a particle filter\n",
    "A particle filter requires that we specify:\n",
    "* A **dynamics or transition function** $f({\\bf {x}}_t)$ that predicts how we expect the world to evolve, which takes \n",
    "${\\bf{x}}^k_t \\rightarrow {\\bf x}^k_{t+1}$ for any given sample ${\\bf{x}}^k_t$.\n",
    "* An **observation function** $g({\\bf x}_{t})$ that predicts what we expect to observe, given a hypothesized state: ${\\bf{x}_t^k} \\rightarrow  \\hat{\\bf y}_t^k $\n",
    "* A **weight function**, that, given a hypothesized observation $\\hat{\\bf y}_t^k$, can be used to *approximate* $p(\\hat{\\bf y}_t|{\\bf y}_t)$. This is performed by computing weights $w_k$ for each particle $\\bf x^k$ and then normalizing to produce the per-sample likelihood:\n",
    "$$p^{(k)}(\\hat{\\bf y}_t|{\\bf y}_t) = \\frac{w_k}{\\sum_j w_j}$$\n",
    "* A set of **prior distributions** that specify our initial guesses for $\\hat{\\bf x}_0$, and allow us to draw the samples $\\{ \\hat{\\bf x}^1_0, \\hat{\\bf x}^2_0, \\dots, \\hat{\\bf x}^{N_s}_0 \\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic filtering\n",
    "We will first implement a basic particle filter that can track a very simple one dimensional time series. This toy problem is easy to understand and work with.\n",
    "\n",
    "Then, we will show how this can generalise to the gesture tracking problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import the things we need\n",
    "from __future__ import print_function, division\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import pfilter\n",
    "pfilter = reload(pfilter)\n",
    "import ipywidgets\n",
    "import IPython\n",
    "import scipy.stats\n",
    "import matplotlib, matplotlib.colors\n",
    "matplotlib.rcParams['figure.figsize'] = (14.0, 8.0)\n",
    "%matplotlib inline\n",
    "from scipy.stats import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.auto_scroll_threshold = 9999;\n",
    "OutputArea.prototype._should_scroll = function(){return false};"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some test data\n",
    "To test the particle filter, we will try and track a very simple, 1D sine wave:\n",
    "$${\\bf x_t} = {\\bf y_t} = \\sin(t)$$\n",
    "\n",
    "This is a simple, smooth process.\n",
    "We will try and estimate our \"hidden\" value $\\bf{x_t}$ via an observed variable $\\bf{y_t}$ which is just $\\bf x_t$ with some noise added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 100 points sampled from a sine wave\n",
    "t = np.linspace(0,20,100).reshape(-1,1)\n",
    "x = np.sin(t)\n",
    "plt.plot(t,x)\n",
    "plt.title(\"Sine wave\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple model\n",
    "We will use a very simple model\n",
    "\n",
    "* **Dynamics**\n",
    "We assume that there are no predictable dynamics, just some Gaussian noise $X_t = X_{t+1} +  N(0,\\sigma_p)$.\n",
    "Note that when we have a stochastic element in a particle filter, we simply draw a sample from a distribution, and treat it as part of the deterministic update. Each particle will therefore get a different draw. We can have any kind of stochastic system here, not just additive noise (as in the Kalman filter); we could easily implement multiplicative noise instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sigma_p = 0.2              # the process noise "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Identity Example\n",
    "def dynamics(x):\n",
    "    # tomorrow is the same as today\n",
    "    # but slightly randomly different\n",
    "    # we literally *add* noise to our previous state\n",
    "    # x_{t+1} = x_t + N(0,\\sigma)\n",
    "    return np.random.normal(x, sigma_p,x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can show what this dynamics model looks like, if we had no resampling step in the filter. This makes predictions without ever having seen any data, as we did with the Kalman filter.\n",
    "\n",
    "We only have a simple state with no deterministic dynamics at all, so the result is just a random walk; our predictive model is not very strong!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def simulate_dynamics(priors, dynamics, steps, n_runs=1):\n",
    "    runs = []\n",
    "    def simulate_run():\n",
    "        # draw samples from the priors\n",
    "        x = np.array([p() for p in priors])\n",
    "        xs = [x]    \n",
    "        for i in range(steps):\n",
    "            x = dynamics(x)\n",
    "            xs.append(x)\n",
    "        return xs    \n",
    "    return np.array([simulate_run() for j in range(n_runs)])\n",
    "    \n",
    "# run the simulation 20 times for 200 time steps each time\n",
    "# we use a normal distribution as our prior on starting value\n",
    "simulated = simulate_dynamics(priors=[norm(0,1).rvs], dynamics=dynamics, steps=200, n_runs=20)   \n",
    "plt.plot(simulated[:,:,0].T, alpha=0.3, c='C0');\n",
    "plt.xlabel(\"Time step\")\n",
    "plt.ylabel(\"X\")\n",
    "plt.title(\"Simulated runs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Observation**\n",
    "We assume that the sensor we measure **is** the value we want to infer, i.e. $\\bf y_t=x_t$, and therefore $g({\\bf x_t})$  is just the identity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def observe(x):\n",
    "    # we observe x directly\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Weighting**\n",
    "We weight samples according to how similar they are to the observed output. We use a simple **heat kernel**:\n",
    "$$w_i = e^{\\left(-\\frac{(y-y^\\prime)^2}{2\\beta^2}\\right)}$$\n",
    "$\\beta$ is a parameter that lets us specify how precise our matching between observation and reality is.\n",
    "\n",
    "Note that this gives more weight to particles that are more similar to the observation: it is a **similarity** function, not a distance function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "beta = 0.25                # the RBF width\n",
    "\n",
    "def weight(true_y, hypothesized_y):\n",
    "    # RBF similarity function       \n",
    "    w = np.exp(-np.sum((hypothesized_y-true_y)**2, axis=1)/(0.5*beta**2))    \n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see what this function looks like, comparing a true value of 0 with values in [-1, 1], $\\beta=0.25$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plot the weight as a function of difference to hypothesized value\n",
    "hypo = np.linspace(-1,1,100).reshape(100,1)\n",
    "plt.plot(hypo.reshape(100,), weight(hypo,[0]))\n",
    "plt.axvline(0,c='C1')\n",
    "plt.xlabel(\"Hypothesized value\")\n",
    "plt.ylabel(\"Unnormalised weight\")\n",
    "plt.title(\"RBF kernel, $\\\\beta$=0.25\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Priors**\n",
    "We assume a very simple prior on ${\\bf x_0}$; that it is normally distributed, mean 0, variance 1, $X_0 \\sim N(0,1)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# we assume that, before seeing any evidence, that the particles are \n",
    "# normally distributed about 0, with std. dev. 1.0\n",
    "\n",
    "def prior(n):\n",
    "    return scipy.stats.norm(0.1).rvs(size=(n,1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Filter creation\n",
    "We use a simple implementation of a particle filter (`pfilter`), which simply takes these functions and the number of particles to use in the sampling process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pfilter = reload(pfilter)\n",
    "pf_simple = pfilter.ParticleFilter(initial=prior, \n",
    "                                    observe_fn=observe,\n",
    "                                    n_particles=200,                                    \n",
    "                                    dynamics_fn=dynamics,\n",
    "                                    weight_fn=weight,                    \n",
    "                                    resample_proportion=0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def run_pfilter(pfilter, inputs):\n",
    "    \"\"\"Apply a particle filter to a time series of inputs,\n",
    "    and return the particles, their weights, and the mean\n",
    "    estimated state\"\"\"    \n",
    "    # reset filter\n",
    "    pfilter.init_filter()\n",
    "    particles, weights, means = [], [], []\n",
    "    # apply to each element of the time series\n",
    "    for i in range(len(inputs)):           \n",
    "        pfilter.update([inputs[i]])\n",
    "        # store the trace of the particle filter\n",
    "        particles.append(pfilter.original_particles)    \n",
    "        weights.append(pfilter.weights)\n",
    "        means.append(pfilter.mean_state)        \n",
    "    return np.array(particles), np.array(weights), np.array(means)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define use a utility function to plot the results of running a particle filter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import particle_utils\n",
    "particle_utils = reload(particle_utils)\n",
    "from particle_utils import plot_pfilter, animate_pfilter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def run_filter(sig=0.1, bet=0.5, noise=-2):\n",
    "    global sigma_p, beta, noise_level, frame\n",
    "    sigma_p = sig\n",
    "    beta = bet\n",
    "    noise_level = 10**noise\n",
    "    noise = np.random.normal(0, noise_level, x.shape)\n",
    "    plt.figure(figsize=(12,7))\n",
    "    y = x + noise\n",
    "    particles, weights, means = run_pfilter(pf_simple, y)\n",
    "    # animation: white particles, green MAP, orange observation, red hidden true, mean cyan\n",
    "    #animate_pfilter(t,x,y,particles,weights,means)\n",
    "    plot_pfilter(t, x, y, particles, weights, means)\n",
    "    %gui tk\n",
    "    plt.title(\"Sigma=%.2f, Beta=%.2f, Noise=%.2e\" % (sigma_p, beta, noise_level))\n",
    "    plt.ylim(-1.5, 1.5)\n",
    "    \n",
    "run_filter(sig=0.1, bet=0.5, noise=-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few parameters to adjust here: let's adjust these interactively to see the effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ipywidgets.interact_manual(run_filter, sig=(0.0, 2, 0.05), bet=(0.1, 5.0, 0.1), noise = (-5.0, 1.0, 0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Things to note\n",
    "* This is a trivial model, but still tracks \"complex\" functions, because it is adapting to observations\n",
    "* Things to adjust:\n",
    "    * noise level\n",
    "    * rbf width beta\n",
    "    * dynamics noise sigma\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A more interesting example\n",
    "Imagine we wanted to infer the **phase** of the oscillator driving this sine wave. The phase variable is not observable, but we can  infer it from the observed oscillation. Furthermore, we want the *unwrapped* phase, i.e. we expect the phase to monotonically increase. We also **drop** some of our data to show how the filter responds when observations are not available; the filter has to *predict* without correction in the absence of observation.\n",
    "\n",
    "<img src=\"imgs/phase.gif\">\n",
    "*[Image credit:1ucasvb]*\n",
    "\n",
    "We can encode these assumptions in our model, then see if the particle filter is able to infer the hidden parameter over time.\n",
    "\n",
    "* **Observation**\n",
    "We postulate an observation model:\n",
    "$${\\bf y_t} = \\sin({\\bf x_t})$$\n",
    "i.e. that what we see is the effect of sine on a hidden variable $\\bf x_t$.\n",
    "Because we defined ${\\bf y_t}=\\sin(t)$, we are actually trying to infer $t$.\n",
    "\n",
    "* **Dynamics**\n",
    "We assume that we have a very simple dynamical system in discrete time, where we have the phase and its first time derivative.\n",
    "$${\\bf x_t} = \\begin{bmatrix}x \\\\ \\dot{x}\\end{bmatrix},$$ and \n",
    "$${\\bf x_{t+1}} = {\\bf x_t} + \\begin{bmatrix}\\dot{x} \\\\ 0 \\end{bmatrix} + N(0, \\Sigma),$$ where $$\\Sigma=\n",
    "\\begin{bmatrix}\n",
    "\\sigma_x & 0 \\\\\n",
    "0 & \\sigma_dx \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "This means that if we start linearly increasing or decreasing at a certain rate, we should expect to keep doing so.\n",
    "\n",
    "* **Priors**\n",
    "We again assume that the initial distribution is normally distributed, with \n",
    "$$\n",
    "{\\bf x_0} \\sim N(0, \\Sigma_0)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Example\n",
    "sigma_x = 0.1\n",
    "sigma_dx = 0.001\n",
    "process_sigmas = [sigma_x, sigma_dx] # how much noise for x and dx    \n",
    "\n",
    "# transition function\n",
    "def linear_dynamics(x):        \n",
    "    nx = np.dot(x,np.array([[1,0],\n",
    "                            [1,1]]))        \n",
    "    nx += np.random.normal(0,process_sigmas,x.shape)\n",
    "    return nx\n",
    "\n",
    "def observe_sin(x):    \n",
    "    # y_t = sin(x_t[0])    \n",
    "    return np.sin(x[:,0:1])\n",
    "\n",
    "def weight_sin(hypothesized_y, true_y):\n",
    "    # RBF similarity function, as we used before     \n",
    "    w = np.exp(-np.sum((hypothesized_y-true_y)**2, axis=1)/(0.5*beta**2))    \n",
    "    return w   \n",
    "\n",
    "def prior_sin(n):\n",
    "    # simple normal prior on the initial state\n",
    "    return np.stack([scipy.stats.norm(0,1).rvs(n), scipy.stats.norm(0, 0.25).rvs(n)]).T  \n",
    "\n",
    "# drop some of the data\n",
    "pfilter = reload(pfilter)\n",
    "pf_sin = pfilter.ParticleFilter(initial=prior_sin, \n",
    "                                observe_fn=observe_sin,\n",
    "                                n_particles=200,\n",
    "                                dynamics_fn=linear_dynamics,\n",
    "                                weight_fn=weight_sin,                    \n",
    "                                resample_proportion=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def run_sinfilter(sig_x=0.1, sig_dx=0.01, bet=0.25, noise=-2):\n",
    "    global sigma_p, beta, noise_level, frame\n",
    "    sigma_x = sig_x\n",
    "    sigma_dx = sig_dx\n",
    "    beta = bet\n",
    "    noise_level = 10**noise\n",
    "    noise = np.random.normal(0,noise_level, x.shape)\n",
    "    plt.figure(figsize=(12,7))\n",
    "    y = x + noise\n",
    "    y[20:40,:] = np.nan\n",
    "    frame = 0\n",
    "    particles, weights, means = run_pfilter(pf_sin, y)\n",
    "    plot_pfilter(t, t, y, particles, weights, means)\n",
    "    plt.title(\"Sigma_x=%.2f, Sigma_dx=%.2f, Beta=%.2f, Noise=%.2e\" % (sigma_x, sigma_dx, beta, noise_level))\n",
    "    \n",
    "ipywidgets.interact_manual(run_sinfilter, sig_x=(0.0, 2, 0.05), sig_dx=(0.0, 0.5, 0.01), \n",
    "                           bet=(0.1, 5.0, 0.1), noise = (-5.0, 1.0, 0.1))    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Things to note\n",
    "* The particle filter was able to infer the hidden state, despite only having a forward model (i.e knowledge of $\\sin(x)$, **not** $\\sin^{-1}(x)$)\n",
    "* It correctly unwrapped phase, because we primed it with the dynamics model to expect values that would increase at a linear rate.\n",
    "* This problem results in *multimodal* distributions, because there are an infinite number of solutions to $y=sin(x)$ because we can add any multiple of $2\\pi$ without changing anything.\n",
    "We can see these as fainter lines on the particle plot.\n",
    "* This means that the particle mean is not actually a good estimate in this case! A better choice might be the most likely particle -- the Maximum A Posteriori (MAP) estimate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resampling\n",
    "One element we have not discussed is the `resample_proportion=0.01` parameter when creating the particle filter.\n",
    "\n",
    "This tells the filter to replace 1% of particles in each time step with random draws from the original prior. There are formulations of the particle filter which omit this, but it is often very useful to introduce this resampling process. \n",
    "\n",
    "If the filter drifts far from the observations, or a step change occurs in the observation (imagine a camera suddenly changing exposure), it may take a long time for the particle filter to adapt. In the worst case, it might never be able to move into the relevant portion of the state space. Prior resampling helps keep a \"bank\" of fresh particles which can rapidly lock onto to new hypotheses.\n",
    "\n",
    "This is very similar in motivation to the use of multiple chains or multiple restarts in MCMC sampling.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge: Gesture recognition\n",
    "Lets now apply these ideas to a practical HCI task: recognising 2D gestures. We will try and  do the full set of gesture recognition tasks is one single, probabilistic model:\n",
    "* **spot** gestures: determine when they start and end, without any external segmentation cue like a button push.\n",
    "* **recognise** gestures: label them according to class\n",
    "* **parameterise** gestures: recover parameters like the size, speed or rotation of the gestures performed.\n",
    "\n",
    "This is a challenging task! We will assume a small set of Graffiti-like symbols as the basis for this example.\n",
    "\n",
    "We will base our algorithm directly on the one given in [A Probabilistic Framework for Matching\n",
    "Temporal Trajectories](http://www.cs.toronto.edu/~jepson/papers/BlackJepsonECCV1998.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "We have some example data with a few example 2D mouse-drawn gestures. Here we are assuming just a **single** template for each gesture, for simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gestures\n",
    "g = gestures.GestureData(\"gestures.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gesture shapes \n",
    "We can plot the shapes of the gesture trajectories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_gestures = g.n_gestures\n",
    "for i in range(n_gestures):\n",
    "    # one plot per gesture\n",
    "    sub = plt.subplot(1, n_gestures,i+1)\n",
    "    path = g.gestures[i]    \n",
    "    plt.plot(path[:,0], path[:,1])\n",
    "    plt.plot(path[0,0], path[0,1], 'o')\n",
    "    \n",
    "    plt.text(0,0,\"%d\"%i)\n",
    "    plt.axis(\"off\")\n",
    "    plt.axis(\"equal\")\n",
    "   \n",
    "    plt.xlim(0,280)\n",
    "    plt.ylim(0,280)\n",
    "    sub.invert_yaxis() # make it appear correctly for screen co-ordinates\n",
    "    plt.suptitle(\"Gesture set\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   ## Timeseries view\n",
    "We can also see each gesture as a trajectory of two coordinates ($x,y$ coordinates) over time. This is closer to the way in which the matching will work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "for i in range(n_gestures):\n",
    "    plt.subplot(n_gestures,2,i*2+1)    \n",
    "    path = g.gestures[i]    \n",
    "    plt.plot(path[:,0], '-C0')\n",
    "    plt.plot(path[:,1], '-C1')\n",
    "    plt.xlabel(\"Samples\")\n",
    "    plt.subplot(n_gestures,2,i*2+2)\n",
    "    plt.plot(path[:,0], path[:,1], 'C0')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.axis(\"equal\")\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get_template\n",
    "We have a simple utility function `get_template(i, t)` which returns the $x,y$ co-ordinate for gesture $i$ at sample $t$. It automatically clips $t$ from 0 to the length of the gesture (in frames).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# test get_template\n",
    "for i in range(100):\n",
    "    xy = g.get_template(1, i)\n",
    "    plt.plot(xy[0], -xy[1], 'C1.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge\n",
    "We want to recognize 2D gestures drawn with a mouse (or finger/stylus). \n",
    "\n",
    "### We know:\n",
    "* We *observe* sequences of $x,y$ coordinates over time.\n",
    "* We have some example templates for particular shapes that we want to match (e.g. letters)\n",
    "\n",
    "### We don't know:\n",
    "* where the user will start drawing a gesture\n",
    "* how big the gesture will be    \n",
    "* how fast the user will draw the gesture (it may well be drawn at a non-constant speed)\n",
    "\n",
    "### We want to know:\n",
    "* which gesture the user is performing, if any\n",
    "* when the user has finished doing the gesture\n",
    "* the parameters of the movement, like speed of performance\n",
    "\n",
    "----- \n",
    "## Your task\n",
    "**You** have to implement the four key elements of the model:\n",
    "* the transition function $f(x_t)$\n",
    "* the observation function $g(x_t)$\n",
    "* the prior samping function $p()$\n",
    "* the weighting function $w(y_t, \\hat{y_t})$\n",
    "\n",
    "The rest of the machinery for recognition is implemented for you.\n",
    "\n",
    "With these four components implemented correctly, you will have a working gesture recogniser.\n",
    "\n",
    "The following description outlines in more detail how such a model would work:\n",
    "   \n",
    "----\n",
    "\n",
    "#### Probabilistic view\n",
    "Putting our assumptions into the probabilistic framework, we want to infer a probability distribution over gesture classes, parameters and gesture completion state, given a time series of $x,y$ coordinates. The $x,y$ points at each time step form our observation vector $y_t$.\n",
    "\n",
    "We *assume* gesture reproduction is in some way a \"noisy reproduction\" of the ideal template form, where there are various types of distortion that can be encountered. We formulate our problem as inferring a distribution over the parameters of these distortions.\n",
    "\n",
    "#### Markov approximation\n",
    "We could look at the whole time series of $x,y$ points and try and classify that. However, there are two problems:\n",
    "* What is the \"whole\" time series -- i.e. how do we segment the gesture?\n",
    "* We would have to store the entire series and somehow match it against templates.\n",
    "\n",
    "This is doable, but a simple way to eliminate these problems is to rewrite the recognizer so that it depends on nothing but its immediately previous state; i.e. so that it satisfies the Markov property.\n",
    "\n",
    "To do this, we need to introduce additional variables into the state space; but with judicious choice these can be a very small number of additional variables. In particular, for this problem, we can just track how far along a gesture we are (the \"phase\") and update that over time. The phase state starts at zero and increments as the gesture is performed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters\n",
    "We are now in a position to write down a model for our gesture recognizer.\n",
    "\n",
    "### State\n",
    "First of all, the state we are trying to infer:\n",
    "\n",
    "<img src=\"imgs/gesture.png\">\n",
    "\n",
    "$${\\bf x_t} = [i,s,x_c,y_c,\\theta,\\phi,\\dot{\\phi}]$$\n",
    "\n",
    "We have one of $n$ possible gestures\n",
    "* $i$ the class index of the gesture.\n",
    "\n",
    "Our model says a gesture will be identical to the template for that class of gesture, but might vary in:\n",
    "* $s$ overall scale, within some tolerance\n",
    "* $x_c,y_c$ center position (could be anywhere on screen)\n",
    "* $\\theta$ small changes in rotation (e.g. $<45^o$)\n",
    "\n",
    "We must take note that what we observe is a position at a **single time point** in a gesture. This means we must estimate how far through a gesture we are.\n",
    "* $\\phi$ the proportion of gesture complete, in the fraction [0,1].\n",
    "* $\\dot{\\phi}$ the rate at which the gesture is being performed (i.e. fast or slow)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "Modify the sections in the code cells below to implement the filter as you see it working. Run the testing cell at the\n",
    "end to bring up the interactive recogniser and see how well you do in recognising the six gestures above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Hints:\n",
    "* Write the **observation** function first, then the **prior**, then the **dynamics**, then adjust the **weight** function if needed.\n",
    "* Test the filter in between to see if it behaves as you might expect it to.\n",
    "* As a start, you might want to try tracking the gestures without allowing any geometric transformations, then add these in.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model specification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation\n",
    "\n",
    "* Given a gesture $i$, we have a template $G_i(\\phi)$, which is returns an $x,y$ point for any value of $\\phi$.\n",
    "\n",
    "* We expect to observe $\\hat{{\\bf y}}=AG_i(\\phi)$, where $A$ is a transformation matrix applying the translation $x_c,y_c$, the scaling $s$ and the rotation $\\theta$.\n",
    "\n",
    "* The utility function `linear_transform()` defined below is a useful component in implementing this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def linear_transform(xys, angle=0.0, scale=1.0, translate=(0,0)):\n",
    "    \"\"\"Takes a an n x 2 array of point `xys` and returns the 2D points transformed by\n",
    "    rotating by `angle` (degrees)\n",
    "    scaling by `scale` (proportional 1.0=no change, 0.5=half, etc.)\n",
    "    translating by `translate` ((x,y) offset)\"\"\"\n",
    "    ca, sa = np.cos(np.radians(angle)), np.sin(np.radians(angle))\n",
    "    rot = np.array([[ca, -sa], \n",
    "                    [sa, ca]])\n",
    "    return np.dot(xys, rot)*scale + np.array(translate)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# demo of linear transform on points distributed in a unit square\n",
    "original = np.random.uniform(0,1, size=(200,2))\n",
    "transformed = linear_transform(original, angle=45, scale=1.5, translate=(-5,2))\n",
    "plt.plot(transformed[:,0], transformed[:,1], '.')\n",
    "plt.plot(original[:,0], original[:,1], '.')\n",
    "plt.axis(\"square\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def gesture_observation(state):\n",
    "    # given an n x d matrix of n particle samples\n",
    "    # return a n x 2 matrix of expected x,y, positions for that gesture model\n",
    "    # dummy code,which returns random results:\n",
    "    # obviously, this should be a transformation of the hidden state passed in!\n",
    "    observation_matrix = np.random.normal(200,20,size=(state.shape[0], 2))\n",
    "    return observation_matrix\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dynamics\n",
    "We then need to specify some simple dynamics. Hint: these should allow the values to slowly change over time (i.e. some random drift), except for the phase $\\phi$ which, by our model, we also expect to steadily increase at the rate $\\dot{\\phi}$. The gesture class should probably not change *during* a gesture!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def gesture_dynamics(prev_states):\n",
    "    # take an n x d array of particle samples\n",
    "    # return an n x d array representing the next states\n",
    "    # dummy code: apply no dynamics\n",
    "    next_states = np.array(prev_states)\n",
    "    return next_states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Priors\n",
    "We then need to define our initial guesses for the state of the system, encoded as prior probability distributions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def gesture_prior(n):\n",
    "    # return an n x d matrix with columns [i, s, x_c, y_c, \\theta, \\phi, \\phi_dot] as an initial guess\n",
    "    # these should call a function draw a value from a distribution\n",
    "    # dummy code: choose a random class and set all other variables to 1.0\n",
    "    return np.stack([\n",
    "        0,  # i\n",
    "        np.ones(n,),  # s\n",
    "        np.zeros(n,), np.zeros(n,), # x_c, y_c\n",
    "        np.zeros(n,),  # \\theta\n",
    "        np.zeros(n,), np.ones(n,)]).T # \\phi, \\phi_dot\n",
    "\n",
    "# print some test samples from the prior\n",
    "print(gesture_prior(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weighting function\n",
    "We again use the simple heat kernel (or RBF):\n",
    "$$w_i = e^{\\left(-\\frac{(y-y^\\prime)^2}{2\\beta^2}\\right)}$$\n",
    "(the specific choice of weighting  function is rarely very important, except if there are particularly unusual states to be compared).\n",
    "\n",
    "The `gesture_beta` parameter may need adjusted.\n",
    "You can change this function to a different similarity measure if you want. Note the assumption that we only observe one $x,y$ location at any time $t$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def gesture_weight(hypothesized, true):\n",
    "    # take a 2D observation (x,y)\n",
    "    # and an n x 2 matrix of observation samples (returned from gesture_observation())\n",
    "    # return the weight for each, representing how similar they are\n",
    "    gesture_beta = 1000.0               # the RBF width\n",
    "\n",
    "    # RBF similarity function       \n",
    "    w = np.exp(-np.sum((hypothesized-true)**2, axis=1)/(0.5*gesture_beta**2))\n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the model\n",
    "Running the cell below will start the gesture recogniser using the functions passed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gestures\n",
    "gestures = reload(gestures)\n",
    "gestures.interactive_recogniser(\n",
    "    dynamics=gesture_dynamics,\n",
    "    observation=gesture_observation,\n",
    "    prior=gesture_prior,\n",
    "    weight=gesture_weight,\n",
    "    gestures=g.gestures)\n",
    "%gui tk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outlook\n",
    "---------------------\n",
    "### Scope and limitations\n",
    "#### Scope\n",
    "* Probabilistic filters can be applied to many problems in HCI. Typically, if a process unfolds over time and there is uncertainty, a probabilistic filter is a strong candidate for inference. \n",
    "\n",
    "* The fact that inference is performed over time is a potential advantage over \"static\" classification approaches, as feedback can be generated on the fly, instead only after completion of an action. \n",
    "\n",
    "* In the specific context of gestures, the ability to infer the start and end-point of gestures can solve the \"segmentation problem\" or \"gesture spotting problem\" that is often awkward and leads to kludges like button presses to segment actions.\n",
    "\n",
    "* Probabilistic motion models can easily be linked to higher-order probabilistic models which infer long-term actions on the part of the user. Because everything is a probability distribution, there is a simple common basis for integrating such models. This, for example, can include language models which estimate a distribution over text that is likely to be entered given both user input and a statistical model of language.\n",
    "\n",
    "#### Limitations\n",
    "* PFs can be computationally intensive to run. \n",
    "* Curse-of-dimensionality can make the attractive simplicity of PFs work poorly in practice as the state space expands (although often better than you might expect).\n",
    "* Sometimes the inverse probability model can be hard to formulate. Conversely, it is sometimes very much easier.\n",
    "* Particle filters are simple and elegant, but inferentially weak.\n",
    "* Kalman filters are rigid and restrictive, but very inferentially efficient.\n",
    "* Hybrid approaches (Ensemble Kalman filter, Unscented Kalman Filter, hybrid particle/Kalman filters, Rao-Blackwellized filters) can trade these qualities off, but they aren't off the shelf solutions (i.e. you need an expert!).\n",
    "\n",
    "\n",
    "### Resources\n",
    "#### Basic\n",
    "* Read the [Condensation paper](http://vision.stanford.edu/teaching/cs231b_spring1415/papers/isard-blake-98.pdf).\n",
    "* Read [the Kalman filter in pictures](http://www.bzarg.com/p/how-a-kalman-filter-works-in-pictures/)\n",
    "* Watch [the particle filter without equations](https://www.youtube.com/watch?v=aUkBa1zMKv4)\n",
    "\n",
    "#### Advanced\n",
    "* [A technical but succinct and clear explanation of the particle filter](http://www.cns.nyu.edu/~eorhan/notes/particle-filtering.pdf)\n",
    "* [A bibliography of particle filter papers](http://www.stats.ox.ac.uk/~doucet/smc_resources.html)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "### Future of probabilistic filtering\n",
    "\n",
    "#### Learned models\n",
    "\n",
    "Much use of probabilistic filters has depended on strong mathematical models of the fundamental process. For example, in rocket science, sophisticated physics models were used to specify the Kalman filters used for stable control. \n",
    "\n",
    "However, it is becoming increasingly possible to **infer** these models from observations. Techniques such as deep learning (for example variational autoencoders or generative adversarial networks) make it possible to learn very sophisticated *generative models* from observations of\n",
    "data.  These models can be dropped into probabilistic filters to produce robust inferential engines for user interaction.\n",
    "\n",
    "#### Example\n",
    "As an illustrative example, we recently built a touch pose estimator to estimate the pose of a finger from a capacitive sensor array (as found on a touch screen). We trained DCNN to predict finger pose from sensor images (inverse model), a separate deconvolutional CNN to predict sensor images from finger poses (forward model) and then fused these using a particle filter.\n",
    "\n",
    "<img src=\"imgs/fwd_inv.png\">\n",
    "\n",
    "This combined model gives substantial robustness, and we were able to introduce a simple dynamics model, which filters out completely implausible movements.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
