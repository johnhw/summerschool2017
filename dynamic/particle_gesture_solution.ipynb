{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imgs/logo.png\" width=\"100%\">\n",
    "-----\n",
    "\n",
    "## Probabilistic filtering for intention inference\n",
    "\n",
    "### Particle filters: Sequential Monte Carlo estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Problem description\n",
    "We are going to solve a simple problem:\n",
    "\n",
    "* Recognising simple 2D gestures, drawn with a mouse.\n",
    "\n",
    "This is readily solved with \"standard\" machine learning algorithms, but we will show how a model-led approach lets us encode our assumptions elegantly **and** we get all the benefits of *probabilistic* tracking. We'll see how probabilistic filters degrade gracefully when our models are bad or measurements are noisy.\n",
    "\n",
    "<img  src=\"imgs/capture.png\" width=\"80%\"/>\n",
    "\n",
    "#### Algorithm\n",
    "We will use the **particle filter** algorithm (technically the **SIR** variant, which is the simplest to understand).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why particle filter?\n",
    "The Kalman filter used the normal distribution to model all of the uncertainty in the system. This is great for efficiency, since the updates are simple linear transforms. But it has several significant drawbacks, which make it difficult to apply directly to infer gestures from observations:\n",
    "\n",
    "#### Kalman filter drawbacks\n",
    "* the **dynamics** have to be linear: we can't have complicated dynamic models (although we can linearise at each time step).\n",
    "\n",
    "This doesn't make much sense for tracking complex gesture trajectories; a dynamic model for a complete gesture is rarely going to be linear.\n",
    "\n",
    "* the **uncertainty** must be normal: so we can't track multiple modes, for example, because a normal distribution has exactly one mode. \n",
    "\n",
    "Imagine an object disappearing behind an obstruction which could reappear on either side; the Kalman filter can only spread out the distribution over the whole area, with an expected location in the middle of the obstacle! We would like to instead be able to track the two possibilities here by splitting up the hypotheses.\n",
    "\n",
    "<img src=\"imgs/landscape.png\">\n",
    "*[Waddington's epigenetic landscape, illustrating a dynamic system which develops multiple modes as it evolves; a Gaussian approximation is wholly inappropriate]*\n",
    "\n",
    "This is critical in gesture recognition for example; at any point in time, our hypotheses might be split among multiple possible gestures with different spatial distributions. Being able to represent the combination of discrete + continuous states is critical. (**Kalman filter banks** are an alternative approach, which explicitly maintain the competing hypotheses as separate Kalman filters)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "### Particle Filtering \n",
    "Particle filters are very simple to understand:\n",
    "\n",
    "* Instead of trying to represent the PDFs of our distributions, which could be any function that integrates to unity, we represent **distributions** using **samples** from those distributions (i.e. it is a **sequential Monte Carlo** method; we use samples to represent distributions). We choose some fixed number of particles $N_s$ and have a set of particles $S_t = \\{ x^{1}_t, x^{2}_t, \\dots, x^{N_s}_t \\}$.\n",
    "* we can apply **any** dynamics simply by applying our dynamics function $f$ to each sample from our current set $S_t$ to get a new predicted set of samples $S_{t+1}$. Likewise, for any sample, we can compute the corresponding observation by applying an arbitrary function $g$ to each sample independently. There are **no** restrictions on the form of $f$ and $g$.\n",
    "* Our uncertainty is captured by the locations of the set of particles in the state space; they are samples from our current posterior. We do not have an explicit parametric form for the distribution; samples are all that we have. Therefore there are no restrictions on the form of the distribution; it can be multi-modal, heavy-tailed etc. \n",
    "* We don't explicitly compute the likelihood of observations, but instead we apply some weighting function to the particles and then normalise these weights to approximate the likelihood. This means we only have to find a reasonable weighting function, and not a true likelihood.\n",
    "* We **resample** particles which are likely after an observation, and discard those which are unlikely. This is **importance resampling**. Without this step, particles would quickly diffuse into very unlikely parts of the space. **Importance resampling** continuously adapts the particles to cover likely portions of the space.\n",
    "\n",
    "**This can only ever approximate the \"true\" distribution; but the Monte Carlo approximation turns out to work surprisingly well, and has good theoretical guarantees. **\n",
    "\n",
    "<img src=\"imgs/particleprocess.png\">\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "<video src=\"videos/anim.mpg.webm\">\n",
    "\n",
    "\n",
    "<video src=\"videos/leafv.mpg.webm\">\n",
    "\n",
    "---\n",
    "\n",
    "It might seem surprising that would work, but in fact for many problems it works very well, and it is statistically sound (under relatively relaxed conditions). It is less computationally efficient than the Kalman filter, and also less inferentially efficient (it is usually less certain for the same amount of data). \n",
    "\n",
    "It has many, many variants and many parameters that can be tweaked. This is flexible, but can be hard to optimize.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Particle filters have many uses in HCI. For example, we have used them extensively to track finger configurations when using capacitive sensors. In this case, we have a finger pose state space (hidden) and a sensor matrix (observed), and the filter estimates pose in real-time.\n",
    "\n",
    "<img src=\"imgs/anglepose.jpg\">\n",
    "[See the AnglePose video](http://www.dcs.gla.ac.uk/~jhw/AnglePose-final.mov)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specifying a particle filter\n",
    "A particle filter requires that we specify:\n",
    "* A **dynamics function** $f({\\bf {x}}_t)$ that predicts how we expect the world to evolve, which takes \n",
    "${\\bf{x}}^k_t \\rightarrow {\\bf x}^k_{t+1}$ for any given sample ${\\bf{x}}^k_t$.\n",
    "* An **observation function** $g({\\bf x}_{t})$ that predicts what we expect to observe, given a hypothesized state: ${\\bf{x}_t^k} \\rightarrow  \\hat{\\bf y}_t^k $\n",
    "* A **weight function**, that, given a hypothesized observation $\\hat{\\bf y}_t^k$, can be used to compute $p(\\hat{\\bf y}_t|{\\bf y}_t)$. This is performed by computing weights $w_k$ for each particle $\\bf x^k$ and then normalizing to produce the per-sample likelihood:\n",
    "$$p^{(k)}(\\hat{\\bf y}_t|{\\bf y}_t) = \\frac{w_k}{\\sum_j w_j}$$\n",
    "* A set of **prior distributions** that specify our initial guesses for $\\hat{\\bf x}_0$, and allow us to draw the samples $\\{ \\hat{\\bf x}^1_0, \\hat{\\bf x}^2_0, \\dots, \\hat{\\bf x}^{N_s}_0 \\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic filtering\n",
    "We will first implement a basic particle filter that can track a very simple one dimensional time series. This toy problem is easy to understand and work with.\n",
    "\n",
    "Then, we will show how this can generalise to an interesting HCI problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import the things we need\n",
    "from __future__ import print_function, division\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import pfilter\n",
    "pfilter = reload(pfilter)\n",
    "import ipywidgets\n",
    "import IPython\n",
    "import scipy.stats\n",
    "import matplotlib, matplotlib.colors\n",
    "matplotlib.rcParams['figure.figsize'] = (14.0, 8.0)\n",
    "%matplotlib inline\n",
    "from scipy.stats import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.auto_scroll_threshold = 9999;\n",
    "OutputArea.prototype._should_scroll = function(){return false};"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some test data\n",
    "To test the particle filter, we will try and track a very simple, 1D sine wave:\n",
    "$$Y_t=\\sin(t)$$\n",
    "\n",
    "This is a simple, smooth process.\n",
    "We will try and estimate our \"hidden\" value $\\bf{X}$ via an observed variable $\\bf{Y}$ which is just $\\bf X$ with some noise added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 100 points sampled from a sine wave\n",
    "t = np.linspace(0,20,100).reshape(-1,1)\n",
    "x = np.sin(t)\n",
    "plt.plot(t,x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple model\n",
    "We will use a very simple model\n",
    "\n",
    "* **Dynamics**\n",
    "We assume that there are no predictable dynamics, just some Gaussian noise $X_t = X_{t+1} +  N(0,\\sigma_p)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sigma_p = 0.2              # the process noise "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Identity Example\n",
    "def dynamics(x):\n",
    "    # tomorrow is the same as today\n",
    "    # but slightly randomly different\n",
    "    # we literally *add* noise to our previous state\n",
    "    return np.random.normal(x, sigma_p, x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can show what this dynamics model looks like, if we had no resampling step in the filter. This makes predictions without ever having seen any data.\n",
    "\n",
    "The result is just a random walk; our predictive model is not very strong!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def simulate_dynamics(priors, dynamics, steps, n_runs=1):\n",
    "    runs = []\n",
    "    def simulate_run():\n",
    "        # draw samples from the priors\n",
    "        x = np.array([p() for p in priors])\n",
    "        xs = [x]    \n",
    "        for i in range(steps):\n",
    "            x = dynamics(x)\n",
    "            xs.append(x)\n",
    "        return xs    \n",
    "    return np.array([simulate_run() for j in range(n_runs)])\n",
    "    \n",
    "# run the simulation 20 times for 200 time steps each time\n",
    "# we use a normal distribution as our prior on starting value\n",
    "simulated = simulate_dynamics(priors=[norm(0,1).rvs], dynamics=dynamics, steps=200, n_runs=20)   \n",
    "plt.plot(simulated[:,:,0].T, alpha=0.3, c='C0');\n",
    "plt.xlabel(\"Time step\")\n",
    "plt.ylabel(\"X\")\n",
    "plt.title(\"Simulated runs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Observation**\n",
    "We assume that the sensor we measure **is** the value we want to infer, i.e. $\\bf y_t=x_t$, and therefore $g({\\bf x_t})$  is just the identity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def observe(x):\n",
    "    # we observe x directly\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Weighting**\n",
    "We weight samples according to how similar they are to the observed output. We use a simple **heat kernel**:\n",
    "$$w_i = e^{\\left(-\\frac{(y-y^\\prime)^2}{2\\beta^2}\\right)}$$\n",
    "$\\beta$ is a parameter that lets us specify how precise our matching between observation and reality is.\n",
    "\n",
    "Note that this gives more weight to particles that are more similar to the observation: it is a **similarity** function, not a distance function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "beta = 0.25                # the RBF width\n",
    "\n",
    "def weight(hypothesized_y, true_y):\n",
    "    # RBF similarity function       \n",
    "    w = np.exp(-np.sum((hypothesized_y-true_y)**2, axis=1)/(0.5*beta**2))    \n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see what this function looks like, comparing a true value of 0 with values in [-1, 1], $\\beta=0.25$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plot the weight as a function of difference to hypothesized value\n",
    "hypo = np.linspace(-1,1,100).reshape(100,1)\n",
    "plt.plot(hypo.reshape(100,), weight(hypo, [0]))\n",
    "plt.axvline(0,c='C1')\n",
    "plt.xlabel(\"Hypothesized value\")\n",
    "plt.ylabel(\"Unnormalised weight\")\n",
    "plt.title(\"RBF kernel, $\\\\beta$=0.25\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Priors**\n",
    "We assume a very simple prior on $X$; that it is normally distributed, mean 0, variance 1, $X_0 \\sim N(0,1)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# we assume that, before seeing any evidence, that the particles are \n",
    "# normally distributed about 0, with std. dev. 1.0\n",
    "\n",
    "def prior(n):\n",
    "    return scipy.stats.norm(0.1).rvs(size=(n,1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Filter creation\n",
    "We use a simple implementation of a particle filter, which simply takes these functions and the number of particles to use in the sampling process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pfilter = reload(pfilter)\n",
    "pf_simple = pfilter.ParticleFilter(initial=prior, \n",
    "                                    observe_fn=observe,\n",
    "                                    n_particles=200,                                    \n",
    "                                    dynamics_fn=dynamics,\n",
    "                                    weight_fn=weight,                    \n",
    "                                    resample_proportion=0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def run_pfilter(pfilter, inputs):\n",
    "    \"\"\"Apply a particle filter to a time series of inputs,\n",
    "    and return the particles, their weights, and the mean\n",
    "    estimated state\"\"\"    \n",
    "    # reset filter\n",
    "    pfilter.init_filter()\n",
    "    particles, weights, means = [], [], []\n",
    "    # apply to each element of the time series\n",
    "    for i in range(len(inputs)):           \n",
    "        pfilter.update([inputs[i]])\n",
    "        particles.append(pfilter.original_particles)    \n",
    "        weights.append(pfilter.weights)\n",
    "        means.append(pfilter.mean_state)        \n",
    "    return np.array(particles), np.array(weights), np.array(means)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a utility function to plot the results of running a particle filter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import particle_utils\n",
    "particle_utils = reload(particle_utils)\n",
    "from particle_utils import plot_pfilter, animate_pfilter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def run_filter(sig=0.1, bet=0.5, noise=-2):\n",
    "    global sigma_p, beta, noise_level, frame\n",
    "    sigma_p = sig\n",
    "    beta = bet\n",
    "    noise_level = 10**noise\n",
    "    noise = np.random.normal(0, noise_level, x.shape)\n",
    "    plt.figure(figsize=(12,7))\n",
    "    y = x + noise\n",
    "    particles, weights, means = run_pfilter(pf_simple, y)\n",
    "    # animation: white particles, green MAP, orange observation, red hidden true, mean cyan\n",
    "    #animate_pfilter(t,x,y,particles,weights,means)\n",
    "    plot_pfilter(t, x, y, particles, weights, means)\n",
    "    %gui tk\n",
    "    plt.title(\"Sigma=%.2f, Beta=%.2f, Noise=%.2e\" % (sigma_p, beta, noise_level))\n",
    "    plt.ylim(-1.5, 1.5)\n",
    "    \n",
    "run_filter(sig=0.1, bet=0.5, noise=-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's adjust these interactively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ipywidgets.interact_manual(run_filter, sig=(0.0, 2, 0.05), bet=(0.1, 5.0, 0.1), noise = (-5.0, 1.0, 0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Things to note\n",
    "* This is a trivial model, but still tracks \"complex\" functions, because it is adapting to observations\n",
    "* Things to adjust:\n",
    "    * noise level\n",
    "    * rbf width beta\n",
    "    * dynamics noise sigma\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A more interesting example\n",
    "Imagine we wanted to infer the **phase** of the oscillator driving this sine wave. The phase variable is not observable, but we can  infer it from the observed oscillation. Furthermore, we want the *unwrapped* phase, i.e. we expect the phase to monotonically increase. We also **drop** some of our data to show how the filter responds when observations are not available; the filter has to *predict* without correction in the absence of observation.\n",
    "\n",
    "<img src=\"imgs/phase.gif\">\n",
    "*[Image credit:1ucasvb]*\n",
    "\n",
    "We can encode these assumptions in our model, then see if the particle filter is able to infer the hidden parameter over time.\n",
    "\n",
    "* **Observation**\n",
    "We postulate an observation model:\n",
    "$$Y_t = \\sin(X_t)$$\n",
    "i.e. that what we see is the effect of sine on a hidden variable $X_t$.\n",
    "Because we defined $Y_t=\\sin(t)$, we are actually trying to infer $t$.\n",
    "\n",
    "* **Dynamics**\n",
    "We assume that we have a very simple dynamical system in discrete time, where we have a velocity and a position.\n",
    "$$X_t = \\begin{bmatrix}x \\\\ \\dot{x}\\end{bmatrix},$$ and \n",
    "$$X_{t+1} = X_t + \\begin{bmatrix}\\dot{x} \\\\ 0 \\end{bmatrix}.$$\n",
    "\n",
    "This means that if we start linearly increasing or decreasing at a certain rate, we should expect to keep doing so.\n",
    "\n",
    "* **Priors**\n",
    "We again assume that the initial distribution is normally distributed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Example\n",
    "sigma_x = 0.1\n",
    "sigma_dx = 0.001\n",
    "def linear_dynamics(x):        \n",
    "    nx = np.dot(x,np.array([[1,0],\n",
    "                            [1,1]]))        \n",
    "    process_sigmas = [sigma_x, sigma_dx] # how much noise for x and dx    \n",
    "    nx += np.random.normal(0,process_sigmas,x.shape)\n",
    "    return nx\n",
    "\n",
    "def observe_sin(x):    \n",
    "    # y = sin(x)    \n",
    "    return np.sin(x[:,0:1])\n",
    "\n",
    "def weight_sin(hypothesized_y, true_y):\n",
    "    # RBF similarity function       \n",
    "    w = np.exp(-np.sum((hypothesized_y-true_y)**2, axis=1)/(0.5*beta**2))    \n",
    "    return w\n",
    "\n",
    "def prior_sin(n):\n",
    "    return np.stack([scipy.stats.norm(0,1).rvs(n), scipy.stats.norm(0, 0.25).rvs(n)]).T  \n",
    "\n",
    "# drop some of the data\n",
    "pfilter = reload(pfilter)\n",
    "pf_sin = pfilter.ParticleFilter(initial=prior_sin, \n",
    "                                observe_fn=observe_sin,\n",
    "                                n_particles=200,\n",
    "                                dynamics_fn=linear_dynamics,\n",
    "                                weight_fn=weight_sin,                    \n",
    "                                resample_proportion=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def run_sinfilter(sig_x=0.1, sig_dx=0.01, bet=0.25, noise=-2):\n",
    "    global sigma_p, beta, noise_level, frame\n",
    "    sigma_x = sig_x\n",
    "    sigma_dx = sig_dx\n",
    "    beta = bet\n",
    "    noise_level = 10**noise\n",
    "    noise = np.random.normal(0,noise_level, x.shape)\n",
    "    plt.figure(figsize=(12,7))\n",
    "    y = x + noise\n",
    "    y[20:40,:] = np.nan\n",
    "    frame = 0\n",
    "    particles, weights, means = run_pfilter(pf_sin, y)\n",
    "    plot_pfilter(t, t, y, particles, weights, means)\n",
    "    plt.title(\"Sigma_x=%.2f, Sigma_dx=%.2f, Beta=%.2f, Noise=%.2e\" % (sigma_x, sigma_dx, beta, noise_level))\n",
    "    \n",
    "ipywidgets.interact_manual(run_sinfilter, sig_x=(0.0, 2, 0.05), sig_dx=(0.0, 0.5, 0.01), \n",
    "                           bet=(0.1, 5.0, 0.1), noise = (-5.0, 1.0, 0.1))    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Things to note\n",
    "* The particle filter was able to infer the hidden state, despite only having a forward model (i.e knowledge of $\\sin(x)$, **not** $\\sin^{-1}(x)$)\n",
    "* It correctly unwrapped phase, because we primed it with the dynamics model to expect values that would increase at a linear rate.\n",
    "* This problem results in *multimodal* distributions, because there are an infinite number of solutions to $y=sin(x)$ because we can add any multiple of $2\\pi$ without changing anything.\n",
    "We can see these as fainter lines on the particle plot.\n",
    "* This means that the particle mean is not actually a good estimate in this case! A better choice might be the most likely particle -- the Maximum A Posteriori (MAP) estimate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge: Gesture recognition\n",
    "Lets now apply these ideas to a practical HCI task: recognising 2D gestures.\n",
    "\n",
    "We will base our algorithm directly on the one given in [A Probabilistic Framework for Matching\n",
    "Temporal Trajectories](http://www.cs.toronto.edu/~jepson/papers/BlackJepsonECCV1998.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "We have some example data with a few example 2D mouse-drawn gestures. Here we are assuming just a **single** template for each gesture, for simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gestures\n",
    "gestures = reload(gestures)\n",
    "g = gestures.GestureData(\"gestures.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gesture shapes \n",
    "We can plot the shapes of the gesture trajectories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_gestures = g.n_gestures\n",
    "for i in range(n_gestures):\n",
    "    plt.subplot(1, n_gestures,i+1)\n",
    "    path = g.gestures[i]    \n",
    "    plt.plot(path[:,0], -path[:,1])\n",
    "    plt.text(0,0,\"%d\"%i)\n",
    "    plt.axis(\"off\")\n",
    "    plt.axis(\"equal\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timeseries view\n",
    "We can also see each gesture as a trajectory of two coordinates ($x,y$ coordinates) over time. This is closer to the way in which the matching will work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "for i in range(n_gestures):\n",
    "    plt.subplot(n_gestures,2,i*2+1)    \n",
    "    path = g.gestures[i]    \n",
    "    plt.plot(path[:,0], 'oC0')\n",
    "    plt.plot(path[:,0], '-C0')\n",
    "    plt.plot(path[:,1], 'oC1')\n",
    "    plt.plot(path[:,1], '-C1')\n",
    "    plt.xlabel(\"Samples\")\n",
    "    plt.ylabel(\"Position\")    \n",
    "    plt.subplot(n_gestures,2,i*2+2)\n",
    "    plt.plot(path[:,0], -path[:,1], 'C0')\n",
    "    plt.axis(\"equal\")\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get_template\n",
    "We have a simple utility function `get_template(i, t)` which returns the $x,y$ co-ordinate for gesture $i$ at sample $t$. It automatically clips $t$ from 0 to the length of the gesture (in frames).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gestures\n",
    "gestures = reload(gestures)\n",
    "g = gestures.GestureData(\"gestures.txt\")\n",
    "\n",
    "for i in range(100):\n",
    "    xy = g.get_template(1, i)\n",
    "    plt.plot(xy[0], -xy[1], 'C1.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge\n",
    "We want to recognize 2D gestures drawn with a mouse (or finger/stylus). \n",
    "* We *observe* sequences of $x,y$ coordinates over time.\n",
    "* We have some example templates for particular shapes that we want to match (e.g. letters)\n",
    "* We don't know:\n",
    "    * where the user will start drawing a gesture\n",
    "    * how big the gesture will be    \n",
    "    * how fast the user will draw the gesture (it may well be drawn at a non-constant speed)\n",
    "* We want to be able to identify:  \n",
    "    * which gesture the user is performing, if any\n",
    "    * when the user has finished doing the gesture\n",
    "   \n",
    "**You** have to implement the four key elements of the model:\n",
    "* the dynamics function $f$\n",
    "* the observation function $g$\n",
    "* the prior samping function $p$\n",
    "* the weighting function $w$\n",
    "\n",
    "With these four components implemented correctly, you will have a working gesture recogniser.\n",
    "\n",
    "The following description outlines in more detail how such a model would work:\n",
    "   \n",
    "#### Probabilistic view\n",
    "Putting this into the probabilistic framework, we want to infer a probability distribution over gesture classes and gesture completion state, given a time series of $x,y$ coordinates. The $x,y$ points form our observation vector $Y$.\n",
    "\n",
    "We assume gesture reproduction is in some way a \"noisy reproduction\" of the ideal template form, where there are various types of distortion that can be encountered.\n",
    "\n",
    "#### Markov approximation\n",
    "We could look at the whole time series of $x,y$ points and try and classify that. However, there are two problems:\n",
    "* What is the \"whole\" time series -- i.e. how do we segment the gesture?\n",
    "* We would have to store the entire series and somehow match it against templates.\n",
    "\n",
    "A simple way to eliminate these problems is to rewrite the recognizer so that it depends on nothing but its immediately previous state; i.e. so that it satisfies the Markov property.\n",
    "\n",
    "To do this, we need to introduce additional variables into the state we are estimating; but with judicious choice these can be a very small number of additional variables. In particular, we can just track how far along a gesture we are (the \"phase\") and update that over time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters\n",
    "We are now in a position to write down a model for our gesture recognizer.\n",
    "\n",
    "### State\n",
    "First of all, the state we are trying to infer:\n",
    "\n",
    "<img src=\"imgs/gesture.png\">\n",
    "\n",
    "$${\\bf x_t} = [i,s,x_c,y_c,\\theta,\\phi,\\dot{\\phi}]$$\n",
    "\n",
    "We have one of $n$ possible gestures\n",
    "* $i$ the number of the gesture\n",
    "\n",
    "Our model says a gesture will be identical to the template for that class of gesture, but might vary in:\n",
    "* $s$ overall scale, within some tolerance\n",
    "* $x_c,y_c$ center position (could be anywhere on screen)\n",
    "* $\\theta$ small changes in rotation (e.g. $<45^o$)\n",
    "\n",
    "We must take note that what we observe is a position at a **single time point** in a gesture. This means we must estimate how far through a gesture we are.\n",
    "* $\\phi$ the proportion of gesture complete, in the fraction [0,1].\n",
    "* $\\dot{\\phi}$ the rate at which the gesture is being performed (i.e. fast or slow)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "Modify the sections in the code cells below to implement the filter as you see it working. Run the testing cell at the\n",
    "end to bring up the interactive recogniser and see how well you do in recognising the six gestures above.\n",
    "\n",
    "### Hints:\n",
    "* Write the **observation** function first, then the **prior**, then the **dynamics**, then adjust the **weight** function if needed.\n",
    "* As a start, you might want to try tracking the gestures without allowing any geometric transformations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation\n",
    "\n",
    "* Given a gesture $i$, we have a template $G_i(\\phi)$, which is returns an $x,y$ point for any value of $\\phi$.\n",
    "\n",
    "* We expect to observe $\\hat{{\\bf y}}=AG_i(\\phi)$, where $A$ is a transformation matrix applying the translation $x_c,y_c$, the scaling $s$ and the rotation $\\theta$.\n",
    "\n",
    "* The utility function `linear_transform()` defined below is a useful component in implementing this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def linear_transform(xys, angle=0.0, scale=1.0, translate=(0,0)):\n",
    "    \"\"\"Takes a an n x 2 array of point `xys` and returns the 2D points transformed by\n",
    "    rotating by `angle` (degrees)\n",
    "    scaling by `scale` (proportional 1.0=no change, 0.5=half, etc.)\n",
    "    translating by `translate` ((x,y) offset)\"\"\"\n",
    "    ca, sa = np.cos(np.radians(angle)), np.sin(np.radians(angle))\n",
    "    rot = np.array([[ca, -sa], \n",
    "                    [sa, ca]])\n",
    "    return np.dot(xys, rot)*scale + np.array(translate)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# demo of linear transform on points distributed in a unit square\n",
    "original = np.random.uniform(0,1, size=(100,2))\n",
    "transformed = linear_transform(original, angle=45, scale=1.5, translate=(-5,2))\n",
    "plt.plot(transformed[:,0], transformed[:,1], 'o')\n",
    "plt.plot(original[:,0], original[:,1], 'o')\n",
    "plt.axis(\"square\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def gesture_observation(state):\n",
    "    # given an n x d matrix of n particle samples\n",
    "    # return a n x 2 matrix of expected x,y, positions for that gesture model\n",
    "    # dummy code,which returns random results:\n",
    "  \n",
    "    transformed = [linear_transform(g.get_template(s[0], s[5]), scale=s[1], angle=s[4], \n",
    "                                     translate=[s[2], s[3]]) for s in state]                 \n",
    "    \n",
    "    \n",
    "    return np.array(transformed)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dynamics\n",
    "We then need to specify some simple dynamics. Hint: these should allow the values to slowly change over time (i.e. some random drift), except for the phase $\\phi$ which, by our model, we also expect to steadily increase at the rate $\\dot{\\phi}$. The gesture class should probably not change *during* a gesture!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def gesture_dynamics(prev_states):\n",
    "    # take an n x d array of particle samples\n",
    "    # return an n x d array representing the next states\n",
    "    # dummy code: apply no dynamics\n",
    "    next_states = np.array(prev_states)\n",
    "    next_states[:,5] += next_states[:,6] \n",
    "    \n",
    "    next_states[:,1] += np.random.normal(0,0.1,next_states[:,1].shape) # scale\n",
    "    next_states[:,2:4] += np.random.normal(0,1,next_states[:,2:4].shape) # pos\n",
    "    next_states[:,4] += np.random.normal(0,0.05,next_states[:,4].shape) # angle\n",
    "    next_states[:,5] += np.random.normal(0,3.0,next_states[:,5].shape)    # phi\n",
    "    next_states[:,6] += np.random.normal(0,0.1,next_states[:,6].shape)  # dphi\n",
    "    \n",
    "    return next_states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Priors\n",
    "We then need to define our initial guesses for the state of the system, encoded as prior probability distributions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def gesture_prior(n):\n",
    "    # return an n x d matrix with columns [i, s, x_c, y_c, \\theta, \\phi, \\phi_dot] as an initial guess\n",
    "    # these should call a function draw a value from a distribution\n",
    "    # dummy code: choose a random class and set all other variables to 1.0\n",
    "    return np.stack([\n",
    "        np.random.randint(0,n_gestures,size=n), \n",
    "        np.random.normal(1.0,0.25,size=n), \n",
    "        np.random.uniform(-200,400,size=n), np.random.uniform(-200,400,size=n),\n",
    "        np.random.normal(0.0, 10.0, size=n), \n",
    "        np.random.normal(0.0, 10.0, size=n), np.random.normal(0.25, 0.1, size=n)]).T\n",
    "\n",
    "# print some test samples from the prior\n",
    "print(gesture_prior(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weighting function\n",
    "We again use the simple heat kernel (or RBF):\n",
    "$$w_i = e^{\\left(-\\frac{(y-y^\\prime)^2}{2\\beta^2}\\right)}$$\n",
    "(the specific choice of weighting  function is rarely very important, except if there are particularly unusual states to be compared).\n",
    "\n",
    "The `gesture_beta` parameter may need adjusted.\n",
    "You can change this function to a different similarity measure if you want. Note the assumption that we only observe one $x,y$ location at any time $t$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def gesture_weight(hypothesized, true):\n",
    "    # take a 2D observation (x,y)\n",
    "    # and an n x 2 matrix of observation samples (returned from gesture_observation())\n",
    "    # return the weight for each, representing how similar they are\n",
    "    gesture_beta = 180.0          # the RBF width\n",
    "    \n",
    "    # RBF similarity function       \n",
    "    w = np.exp(-np.sum((hypothesized-true)**2, axis=1)/(0.5*gesture_beta**2))\n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the model\n",
    "Running the cell below will start the gesture recogniser using the functions passed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gestures\n",
    "gestures = reload(gestures)\n",
    "gestures.interactive_recogniser(\n",
    "    dynamics=gesture_dynamics,\n",
    "    observation=gesture_observation,\n",
    "    prior=gesture_prior,\n",
    "    weight=gesture_weight,\n",
    "    gestures=g.gestures)\n",
    "%gui tk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outlook\n",
    "---------------------\n",
    "### Scope and limitations\n",
    "#### Scope\n",
    "* Probabilistic filters can be applied to many problems in HCI. Typically, if a process unfolds over time and there is uncertainty, a probabilistic filter is a strong candidate for inference. \n",
    "\n",
    "* The fact that inference is performed over time is a potential advantage over \"static\" classification approaches, as feedback can be generated on the fly, instead only after completion of an action. \n",
    "\n",
    "* In the specific context of gestures, the ability to infer the start and end-point of gestures can solve the \"segmentation problem\" or \"gesture spotting problem\" that is often awkward and leads to kludges like button presses to segment actions.\n",
    "\n",
    "* Probabilistic motion models can easily be linked to higher-order probabilistic models which infer long-term actions on the part of the user. Because everything is a probability distribution, there is a simple common basis for integrating such models. This, for example, can include language models which estimate a distribution over text that is likely to be entered given both user input and a statistical model of language.\n",
    "\n",
    "#### Limitations\n",
    "* PFs can be computationally intensive to run. \n",
    "* Curse-of-dimensionality can make the attractive simplicity of PFs work poorly in practice as the state space expands (although often better than you might expect).\n",
    "* Sometimes the inverse probability model can be hard to formulate. Conversely, it is sometimes very much easier.\n",
    "* Particle filters are simple and elegant, but inferentially weak.\n",
    "* Kalman filters are rigid and restrictive, but very inferentially efficient.\n",
    "* Hybrid approaches (Ensemble Kalman filter, Unscented Kalman Filter, hybrid particle/Kalman filters, Rao-Blackwellized filters) can trade these qualities off, but they aren't off the shelf solutions (i.e. you need an expert!).\n",
    "\n",
    "\n",
    "### Resources\n",
    "#### Basic\n",
    "* Read the [Condensation paper](http://vision.stanford.edu/teaching/cs231b_spring1415/papers/isard-blake-98.pdf).\n",
    "* Read [the Kalman filter in pictures](http://www.bzarg.com/p/how-a-kalman-filter-works-in-pictures/)\n",
    "* Watch [the particle filter without equations](https://www.youtube.com/watch?v=aUkBa1zMKv4)\n",
    "\n",
    "#### Advanced\n",
    "* [A technical but succinct and clear explanation of the particle filter](http://www.cns.nyu.edu/~eorhan/notes/particle-filtering.pdf)\n",
    "* [A bibliography of particle filter papers](http://www.stats.ox.ac.uk/~doucet/smc_resources.html)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "### Future of probabilistic filtering\n",
    "\n",
    "#### Learned models\n",
    "\n",
    "Much use of probabilistic filters has depended on strong mathematical models of the fundamental process. For example, in rocket science, sophisticated physics models were used to specify the Kalman filters used for stable control. \n",
    "\n",
    "However, it is becoming increasingly possible to **infer** these models from observations. Techniques such as deep learning (for example variational autoencoders or generative adversarial networks) make it possible to learn very sophisticated *generative models* from observations of\n",
    "data.  These models can be dropped into probabilistic filters to produce robust inferential engines for user interaction.\n",
    "\n",
    "#### Example\n",
    "As an illustrative example, we recently built a touch pose estimator to estimate the pose of a finger from a capacitive sensor array (as found on a touch screen). We trained DCNN to predict finger pose from sensor images (inverse model), a separate deconvolutional CNN to predict sensor images from finger poses (forward model) and then fused these using a particle filter.\n",
    "\n",
    "<img src=\"imgs/fwd_inv.png\">\n",
    "\n",
    "This combined model gives substantial robustness, and we were able to introduce a simple dynamics model, which filters out completely implausible movements.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
